{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7tuzw4mQq8A"
      },
      "source": [
        "---\n",
        "# Multiple Choice Question-Answering in a k-shot Setting with GPT-2 Variants.\n",
        "---\n",
        "## 1. Background:\n",
        "\n",
        "Modern transformer-based language models, particularly the GPT variants, have showcased impressive capabilities in understanding and generating human-like text. One of the promising applications of such models is in the realm of question-answering. Our goal is to explore the potential of these models in a k-shot learning setup for multiple choice questions.\n",
        "\n",
        "### 1.1. What is k-shot Learning?\n",
        "\n",
        "In few-shot learning, \"k-shot\" refers to using k examples to instruct a model. For transformer-based models, this involves \"priming\" or \"prompting\" the model by presenting it with k examples of a particular task. These examples set the context and hint the model about the nature of the task it's expected to perform. The idea is that by observing k examples, the model can better understand and execute subsequent tasks.\n",
        "\n",
        "### 1.2. Expected Prompt Structure\n",
        "\n",
        "Given the k-shot setting, your prompts to the model should be structured as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akWXopefRLSC"
      },
      "source": [
        "```\n",
        "The following are multiple choice questions (with answers) about [topic].\n",
        "\n",
        "Question: [Question 1]\n",
        "A. [Option 1.1]\n",
        "B. [Option 1.2]\n",
        "...\n",
        "Answer: [Correct Answer for Question 1]\n",
        "\n",
        "...\n",
        "\n",
        "Question: [Question k]\n",
        "A. [Option k.1]\n",
        "B. [Option k.2]\n",
        "...\n",
        "Answer: [Correct Answer for Question k]\n",
        "\n",
        "Question: [Your target question here]\n",
        "A. [Option 1]\n",
        "B. [Option 2]\n",
        "C. [Option 3]\n",
        "D. [Option 4]\n",
        "Answer:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. How do we actually evaluate the model based on its outputs? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **1.3.1. First approach.** compare the probabilities of the letter answers: A, B, C, D. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "![alt text](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-05.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **1.3.2. Second approach.** The model is expected to generate as text the correct letter answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-06.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. The Data\n",
        "\n",
        "Your dataset (under ```/data```) consists of several CSV files, each pertaining to a different topic. Each CSV file contains multiple choice questions, their options, and the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. The Models\n",
        "\n",
        "For this task, we will use two GPT-2 versions from the Hugging Face library: \"gpt-small\" and \"gpt-medium\". \n",
        "\n",
        "The following is a sample code of how to load the models and their tokenizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "# Necessary imports\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load GPT-2 variants\n",
        "MODEL_NAME_SMALL = \"gpt2\" # For the smaller GPT-2 model\n",
        "MODEL_NAME_LARGE = \"gpt2-medium\" # For the medium-sized GPT-2 model\n",
        "\n",
        "tokenizer_small = GPT2Tokenizer.from_pretrained(MODEL_NAME_SMALL)\n",
        "model_small = GPT2LMHeadModel.from_pretrained(MODEL_NAME_SMALL)\n",
        "\n",
        "tokenizer_large = GPT2Tokenizer.from_pretrained(MODEL_NAME_LARGE)\n",
        "model_large = GPT2LMHeadModel.from_pretrained(MODEL_NAME_LARGE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## 3. Your Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **3.1. Data Preparation and Model Prompting**\n",
        "\n",
        "Extract questions, options, and answers from the dataset and structure them according to the k-shot setting.\n",
        "\n",
        "For each CSV in ```/data```: \n",
        "- The first K rows + filename should be used to build the prompt\n",
        "- The remaining rows should be used to evaluate the model's performance. \n",
        "\n",
        "Prime the model with the first k questions and their answers, then ask the subsequent target question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is a sample function for getting the model's output logits (given a prompt)\n",
        "\n",
        "```python\n",
        "def get_logits(model, tokenizer, prompt):\n",
        "    \n",
        "    # Tokenizing the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "\n",
        "    # Getting model's output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Extract logits corresponding to the position of the options in the prompt\n",
        "    # Assuming the last token's logits correspond to the model's selection among options\n",
        "    logits = outputs.logits[0][-1]\n",
        "\n",
        "    return logits "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As part of the task, **you will have to ensure to manage the token limits of the model, especially when constructing long prompts with larger k values.** \n",
        "\n",
        "If the k-shot prompt is too long token-wise (let's say it's 1024 from now on), then you should construct a k'-shot prompt with k' < k (the largest possible k' that is less than 1024). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **3.2. Analysis**\n",
        "\n",
        "Explore and comment on:\n",
        "   - The two model's performance (accuracy-wise) variability across different topics (and also as a whole) and evaluation methods as described in 1.3. \n",
        "   - The impact of changing the value of k on model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Happy Coding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
